//go:build !noasm && amd64
// Code generated by gocc devel -- DO NOT EDIT.
//
// Source file         : ascii-avx2.c
// Clang version       : Apple clang version 16.0.0 (clang-1600.0.26.4)
// Target architecture : amd64
// Compiler options    : -mavx2 -mfma

#include "textflag.h"

TEXT Â·isAsciiAvx(SB), NOSPLIT, $0-17
	MOVQ src+0(FP), DI
	MOVQ src_len+8(FP), SI
	NOP                      // (skipped)                            // push	rbp
	NOP                      // (skipped)                            // mov	rbp, rsp
	NOP                      // (skipped)                            // and	rsp, -8
	XORL CX, CX              // <--                                  // xor	ecx, ecx
	CMPQ SI, $0x20           // <--                                  // cmp	rsi, 32
	JB   LBB0_1              // <--                                  // jb	.LBB0_1
	LEAQ -0x20(SI), AX       // <--                                  // lea	rax, [rsi - 32]
	MOVQ AX, R9              // <--                                  // mov	r9, rax
	SHRQ $0x5, R9            // <--                                  // shr	r9, 5
	INCQ R9                  // <--                                  // inc	r9
	WORD $0x8944; BYTE $0xca // MOVL R9, DX                          // mov	edx, r9d
	WORD $0xe283; BYTE $0x07 // ANDL $0x7, DX                        // and	edx, 7
	CMPQ AX, $0xe0           // <--                                  // cmp	rax, 224
	JAE  LBB0_4              // <--                                  // jae	.LBB0_4
	LONG $0xc0eff9c5         // VPXOR X0, X0, X0                     // vpxor	xmm0, xmm0, xmm0
	XORL R8, R8              // <--                                  // xor	r8d, r8d
	JMP  LBB0_6              // <--                                  // jmp	.LBB0_6

LBB0_1:
	LONG $0xc0eff9c5 // VPXOR X0, X0, X0                     // vpxor	xmm0, xmm0, xmm0
	XORL AX, AX      // <--                                  // xor	eax, eax
	MOVQ SI, DX      // <--                                  // mov	rdx, rsi
	SUBQ AX, DX      // <--                                  // sub	rdx, rax
	JA   LBB0_11     // <--                                  // ja	.LBB0_11
	JMP  LBB0_25     // <--                                  // jmp	.LBB0_25

LBB0_4:
	ANDQ $-0x8, R9   // <--                                  // and	r9, -8
	LONG $0xc0eff9c5 // VPXOR X0, X0, X0                     // vpxor	xmm0, xmm0, xmm0
	XORL R8, R8      // <--                                  // xor	r8d, r8d

LBB0_5:
	LONG $0xeb7da1c4; WORD $0x0704             // VPOR 0(DI)(R8*1), Y0, Y0             // vpor	ymm0, ymm0, ymmword ptr [rdi + r8]
	LONG $0xeb7da1c4; WORD $0x0744; BYTE $0x20 // VPOR 0x20(DI)(R8*1), Y0, Y0          // vpor	ymm0, ymm0, ymmword ptr [rdi + r8 + 32]
	LONG $0xeb7da1c4; WORD $0x0744; BYTE $0x40 // VPOR 0x40(DI)(R8*1), Y0, Y0          // vpor	ymm0, ymm0, ymmword ptr [rdi + r8 + 64]
	LONG $0xeb7da1c4; WORD $0x0744; BYTE $0x60 // VPOR 0x60(DI)(R8*1), Y0, Y0          // vpor	ymm0, ymm0, ymmword ptr [rdi + r8 + 96]
	QUAD $0x00800784eb7da1c4; WORD $0x0000     // VPOR 0x80(DI)(R8*1), Y0, Y0          // vpor	ymm0, ymm0, ymmword ptr [rdi + r8 + 128]
	QUAD $0x00a00784eb7da1c4; WORD $0x0000     // VPOR 0xa0(DI)(R8*1), Y0, Y0          // vpor	ymm0, ymm0, ymmword ptr [rdi + r8 + 160]
	QUAD $0x00c00784eb7da1c4; WORD $0x0000     // VPOR 0xc0(DI)(R8*1), Y0, Y0          // vpor	ymm0, ymm0, ymmword ptr [rdi + r8 + 192]
	QUAD $0x00e00784eb7da1c4; WORD $0x0000     // VPOR 0xe0(DI)(R8*1), Y0, Y0          // vpor	ymm0, ymm0, ymmword ptr [rdi + r8 + 224]
	ADDQ $0x100, R8                            // <--                                  // add	r8, 256
	ADDQ $-0x8, R9                             // <--                                  // add	r9, -8
	JNE  LBB0_5                                // <--                                  // jne	.LBB0_5

LBB0_6:
	ANDQ $-0x20, AX          // <--                                  // and	rax, -32
	WORD $0x8548; BYTE $0xd2 // TESTQ DX, DX                         // test	rdx, rdx
	JE   LBB0_9              // <--                                  // je	.LBB0_9
	ADDQ DI, R8              // <--                                  // add	r8, rdi
	SHLQ $0x5, DX            // <--                                  // shl	rdx, 5
	XORL R9, R9              // <--                                  // xor	r9d, r9d

LBB0_8:
	LONG $0xeb7d81c4; WORD $0x0804 // VPOR 0(R8)(R9*1), Y0, Y0             // vpor	ymm0, ymm0, ymmword ptr [r8 + r9]
	ADDQ $0x20, R9                 // <--                                  // add	r9, 32
	CMPQ DX, R9                    // <--                                  // cmp	rdx, r9
	JNE  LBB0_8                    // <--                                  // jne	.LBB0_8

LBB0_9:
	ADDQ $0x20, AX // <--                                  // add	rax, 32
	MOVQ SI, DX    // <--                                  // mov	rdx, rsi
	SUBQ AX, DX    // <--                                  // sub	rdx, rax
	JBE  LBB0_25   // <--                                  // jbe	.LBB0_25

LBB0_11:
	CMPQ DX, $0x10 // <--                                  // cmp	rdx, 16
	JAE  LBB0_13   // <--                                  // jae	.LBB0_13
	XORL R8, R8    // <--                                  // xor	r8d, r8d
	JMP  LBB0_23   // <--                                  // jmp	.LBB0_23

LBB0_13:
	CMPQ DX, $0x80 // <--                                  // cmp	rdx, 128
	JAE  LBB0_18   // <--                                  // jae	.LBB0_18
	XORL R8, R8    // <--                                  // xor	r8d, r8d
	XORL CX, CX    // <--                                  // xor	ecx, ecx
	JMP  LBB0_15   // <--                                  // jmp	.LBB0_15

LBB0_18:
	MOVQ DX, CX          // <--                                  // mov	rcx, rdx
	ANDQ $-0x80, CX      // <--                                  // and	rcx, -128
	LEAQ 0(AX)(DI*1), R8 // <--                                  // lea	r8, [rax + rdi]
	ADDQ $0x60, R8       // <--                                  // add	r8, 96
	LONG $0xc9eff1c5     // VPXOR X1, X1, X1                     // vpxor	xmm1, xmm1, xmm1
	XORL R9, R9          // <--                                  // xor	r9d, r9d
	LONG $0xd2efe9c5     // VPXOR X2, X2, X2                     // vpxor	xmm2, xmm2, xmm2
	LONG $0xdbefe1c5     // VPXOR X3, X3, X3                     // vpxor	xmm3, xmm3, xmm3
	LONG $0xe4efd9c5     // VPXOR X4, X4, X4                     // vpxor	xmm4, xmm4, xmm4

LBB0_19:
	LONG $0xeb7581c4; WORD $0x084c; BYTE $0xa0 // VPOR -0x60(R8)(R9*1), Y1, Y1         // vpor	ymm1, ymm1, ymmword ptr [r8 + r9 - 96]
	LONG $0xeb6d81c4; WORD $0x0854; BYTE $0xc0 // VPOR -0x40(R8)(R9*1), Y2, Y2         // vpor	ymm2, ymm2, ymmword ptr [r8 + r9 - 64]
	LONG $0xeb6581c4; WORD $0x085c; BYTE $0xe0 // VPOR -0x20(R8)(R9*1), Y3, Y3         // vpor	ymm3, ymm3, ymmword ptr [r8 + r9 - 32]
	LONG $0xeb5d81c4; WORD $0x0824             // VPOR 0(R8)(R9*1), Y4, Y4             // vpor	ymm4, ymm4, ymmword ptr [r8 + r9]
	SUBQ $-0x80, R9                            // <--                                  // sub	r9, -128
	CMPQ CX, R9                                // <--                                  // cmp	rcx, r9
	JNE  LBB0_19                               // <--                                  // jne	.LBB0_19
	LONG $0xc9ebedc5                           // VPOR Y1, Y2, Y1                      // vpor	ymm1, ymm2, ymm1
	LONG $0xd3ebddc5                           // VPOR Y3, Y4, Y2                      // vpor	ymm2, ymm4, ymm3
	LONG $0xc9ebedc5                           // VPOR Y1, Y2, Y1                      // vpor	ymm1, ymm2, ymm1
	LONG $0x397de3c4; WORD $0x01ca             // VEXTRACTI128 $0x1, Y1, X2            // vextracti128	xmm2, ymm1, 1
	LONG $0xcaebf1c5                           // VPOR X2, X1, X1                      // vpor	xmm1, xmm1, xmm2
	LONG $0xd170f9c5; BYTE $0xee               // VPSHUFD $-0x12, X1, X2               // vpshufd	xmm2, xmm1, 238
	LONG $0xcaebf1c5                           // VPOR X2, X1, X1                      // vpor	xmm1, xmm1, xmm2
	LONG $0xd170f9c5; BYTE $0x55               // VPSHUFD $0x55, X1, X2                // vpshufd	xmm2, xmm1, 85
	LONG $0xcaebf1c5                           // VPOR X2, X1, X1                      // vpor	xmm1, xmm1, xmm2
	LONG $0xd172e9c5; BYTE $0x10               // VPSRLD $0x10, X1, X2                 // vpsrld	xmm2, xmm1, 16
	LONG $0xcaebf1c5                           // VPOR X2, X1, X1                      // vpor	xmm1, xmm1, xmm2
	LONG $0xd171e9c5; BYTE $0x08               // VPSRLW $0x8, X1, X2                  // vpsrlw	xmm2, xmm1, 8
	LONG $0xcaebf1c5                           // VPOR X2, X1, X1                      // vpor	xmm1, xmm1, xmm2
	LONG $0x7e79c1c4; BYTE $0xc8               // VMOVD X1, R8                         // vmovd	r8d, xmm1
	CMPQ DX, CX                                // <--                                  // cmp	rdx, rcx
	JE   LBB0_24                               // <--                                  // je	.LBB0_24
	WORD $0xc2f6; BYTE $0x70                   // TESTL $0x70, DL                      // test	dl, 112
	JE   LBB0_22                               // <--                                  // je	.LBB0_22

LBB0_15:
	MOVQ DX, R9                  // <--                                  // mov	r9, rdx
	ANDQ $-0x10, R9              // <--                                  // and	r9, -16
	LONG $0xc0b60f45             // MOVZX R8, R8                         // movzx	r8d, r8b
	LONG $0x6e79c1c4; BYTE $0xc8 // VMOVD R8, X1                         // vmovd	xmm1, r8d
	LEAQ 0(DI)(AX*1), R8         // <--                                  // lea	r8, [rdi + rax]
	ADDQ R9, AX                  // <--                                  // add	rax, r9

LBB0_16:
	LONG $0xeb71c1c4; WORD $0x080c // VPOR 0(R8)(CX*1), X1, X1             // vpor	xmm1, xmm1, xmmword ptr [r8 + rcx]
	ADDQ $0x10, CX                 // <--                                  // add	rcx, 16
	CMPQ R9, CX                    // <--                                  // cmp	r9, rcx
	JNE  LBB0_16                   // <--                                  // jne	.LBB0_16
	LONG $0xd170f9c5; BYTE $0xee   // VPSHUFD $-0x12, X1, X2               // vpshufd	xmm2, xmm1, 238
	LONG $0xcaebf1c5               // VPOR X2, X1, X1                      // vpor	xmm1, xmm1, xmm2
	LONG $0xd170f9c5; BYTE $0x55   // VPSHUFD $0x55, X1, X2                // vpshufd	xmm2, xmm1, 85
	LONG $0xcaebf1c5               // VPOR X2, X1, X1                      // vpor	xmm1, xmm1, xmm2
	LONG $0xd172e9c5; BYTE $0x10   // VPSRLD $0x10, X1, X2                 // vpsrld	xmm2, xmm1, 16
	LONG $0xcaebf1c5               // VPOR X2, X1, X1                      // vpor	xmm1, xmm1, xmm2
	LONG $0xd171e9c5; BYTE $0x08   // VPSRLW $0x8, X1, X2                  // vpsrlw	xmm2, xmm1, 8
	LONG $0xcaebf1c5               // VPOR X2, X1, X1                      // vpor	xmm1, xmm1, xmm2
	LONG $0x7e79c1c4; BYTE $0xc8   // VMOVD X1, R8                         // vmovd	r8d, xmm1
	CMPQ DX, R9                    // <--                                  // cmp	rdx, r9
	JNE  LBB0_23                   // <--                                  // jne	.LBB0_23
	JMP  LBB0_24                   // <--                                  // jmp	.LBB0_24

LBB0_22:
	ADDQ CX, AX // <--                                  // add	rax, rcx

LBB0_23:
	ORB  0(DI)(AX*1), R8 // <--                                  // or	r8b, byte ptr [rdi + rax]
	INCQ AX              // <--                                  // inc	rax
	CMPQ SI, AX          // <--                                  // cmp	rsi, rax
	JNE  LBB0_23         // <--                                  // jne	.LBB0_23

LBB0_24:
	LONG $0x80e08041 // ANDL $0x80, R8                       // and	r8b, -128
	LONG $0xc8b60f41 // MOVZX R8, CX                         // movzx	ecx, r8b

LBB0_25:
	LONG $0xc0d7fdc5         // VPMOVMSKB Y0, AX                     // vpmovmskb	eax, ymm0
	WORD $0xc809             // ORL CX, AX                           // or	eax, ecx
	WORD $0x940f; BYTE $0xc0 // SETE AL                              // sete	al
	NOP                      // (skipped)                            // mov	rsp, rbp
	NOP                      // (skipped)                            // pop	rbp
	VZEROUPPER               // <--                                  // vzeroupper
	MOVB AX, ret+16(FP)      // <--
	RET                      // <--                                  // ret
