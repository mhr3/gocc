//go:build !noasm && amd64
// Code generated by gocc devel -- DO NOT EDIT.
//
// Source file         : ascii-avx2.c
// Clang version       : Apple clang version 17.0.0 (clang-1700.4.4.1)
// Target architecture : amd64
// Compiler options    : -mavx2 -mfma

#include "textflag.h"

TEXT Â·isAsciiAvx(SB), NOSPLIT, $0-17
	MOVQ src+0(FP), DI
	MOVQ src_len+8(FP), SI
	NOP                      // (skipped)                            // push	rbp
	NOP                      // (skipped)                            // mov	rbp, rsp
	NOP                      // (skipped)                            // and	rsp, -8
	XORL DX, DX              // <--                                  // xor	edx, edx
	CMPQ SI, $0x20           // <--                                  // cmp	rsi, 32
	JB   LBB0_1              // <--                                  // jb	.LBB0_1
	LEAQ -0x20(SI), AX       // <--                                  // lea	rax, [rsi - 32]
	MOVQ AX, R9              // <--                                  // mov	r9, rax
	SHRQ $0x5, R9            // <--                                  // shr	r9, 5
	INCQ R9                  // <--                                  // inc	r9
	WORD $0x8944; BYTE $0xc9 // MOVL R9, CX                          // mov	ecx, r9d
	WORD $0xe183; BYTE $0x07 // ANDL $0x7, CX                        // and	ecx, 7
	CMPQ AX, $0xe0           // <--                                  // cmp	rax, 224
	JAE  LBB0_4              // <--                                  // jae	.LBB0_4
	LONG $0xc0eff9c5         // VPXOR X0, X0, X0                     // vpxor	xmm0, xmm0, xmm0
	XORL R8, R8              // <--                                  // xor	r8d, r8d
	JMP  LBB0_6              // <--                                  // jmp	.LBB0_6

LBB0_1:
	XORL CX, CX  // <--                                  // xor	ecx, ecx
	XORL AX, AX  // <--                                  // xor	eax, eax
	MOVQ SI, R8  // <--                                  // mov	r8, rsi
	SUBQ AX, R8  // <--                                  // sub	r8, rax
	JA   LBB0_11 // <--                                  // ja	.LBB0_11
	JMP  LBB0_25 // <--                                  // jmp	.LBB0_25

LBB0_4:
	ANDQ $-0x8, R9   // <--                                  // and	r9, -8
	LONG $0xc0eff9c5 // VPXOR X0, X0, X0                     // vpxor	xmm0, xmm0, xmm0
	XORL R8, R8      // <--                                  // xor	r8d, r8d

LBB0_5:
	LONG $0xeb7da1c4; WORD $0x0704             // VPOR 0(DI)(R8*1), Y0, Y0             // vpor	ymm0, ymm0, ymmword ptr [rdi + r8]
	LONG $0xeb7da1c4; WORD $0x0744; BYTE $0x20 // VPOR 0x20(DI)(R8*1), Y0, Y0          // vpor	ymm0, ymm0, ymmword ptr [rdi + r8 + 32]
	LONG $0xeb7da1c4; WORD $0x0744; BYTE $0x40 // VPOR 0x40(DI)(R8*1), Y0, Y0          // vpor	ymm0, ymm0, ymmword ptr [rdi + r8 + 64]
	LONG $0xeb7da1c4; WORD $0x0744; BYTE $0x60 // VPOR 0x60(DI)(R8*1), Y0, Y0          // vpor	ymm0, ymm0, ymmword ptr [rdi + r8 + 96]
	QUAD $0x00800784eb7da1c4; WORD $0x0000     // VPOR 0x80(DI)(R8*1), Y0, Y0          // vpor	ymm0, ymm0, ymmword ptr [rdi + r8 + 128]
	QUAD $0x00a00784eb7da1c4; WORD $0x0000     // VPOR 0xa0(DI)(R8*1), Y0, Y0          // vpor	ymm0, ymm0, ymmword ptr [rdi + r8 + 160]
	QUAD $0x00c00784eb7da1c4; WORD $0x0000     // VPOR 0xc0(DI)(R8*1), Y0, Y0          // vpor	ymm0, ymm0, ymmword ptr [rdi + r8 + 192]
	QUAD $0x00e00784eb7da1c4; WORD $0x0000     // VPOR 0xe0(DI)(R8*1), Y0, Y0          // vpor	ymm0, ymm0, ymmword ptr [rdi + r8 + 224]
	ADDQ $0x100, R8                            // <--                                  // add	r8, 256
	ADDQ $-0x8, R9                             // <--                                  // add	r9, -8
	JNE  LBB0_5                                // <--                                  // jne	.LBB0_5

LBB0_6:
	ANDQ $-0x20, AX          // <--                                  // and	rax, -32
	WORD $0x8548; BYTE $0xc9 // TESTQ CX, CX                         // test	rcx, rcx
	JE   LBB0_9              // <--                                  // je	.LBB0_9
	ADDQ DI, R8              // <--                                  // add	r8, rdi
	WORD $0xe1c1; BYTE $0x05 // SHLL $0x5, CX                        // shl	ecx, 5
	XORL R9, R9              // <--                                  // xor	r9d, r9d

LBB0_8:
	LONG $0xeb7d81c4; WORD $0x0804 // VPOR 0(R8)(R9*1), Y0, Y0             // vpor	ymm0, ymm0, ymmword ptr [r8 + r9]
	ADDQ $0x20, R9                 // <--                                  // add	r9, 32
	CMPQ CX, R9                    // <--                                  // cmp	rcx, r9
	JNE  LBB0_8                    // <--                                  // jne	.LBB0_8

LBB0_9:
	ADDQ $0x20, AX   // <--                                  // add	rax, 32
	LONG $0xc8d7fdc5 // VPMOVMSKB Y0, CX                     // vpmovmskb	ecx, ymm0
	MOVQ SI, R8      // <--                                  // mov	r8, rsi
	SUBQ AX, R8      // <--                                  // sub	r8, rax
	JBE  LBB0_25     // <--                                  // jbe	.LBB0_25

LBB0_11:
	CMPQ R8, $0x8 // <--                                  // cmp	r8, 8
	JAE  LBB0_13  // <--                                  // jae	.LBB0_13
	XORL R9, R9   // <--                                  // xor	r9d, r9d
	JMP  LBB0_23  // <--                                  // jmp	.LBB0_23

LBB0_13:
	CMPQ R8, $0x80 // <--                                  // cmp	r8, 128
	JAE  LBB0_18   // <--                                  // jae	.LBB0_18
	XORL R9, R9    // <--                                  // xor	r9d, r9d
	XORL DX, DX    // <--                                  // xor	edx, edx
	JMP  LBB0_15   // <--                                  // jmp	.LBB0_15

LBB0_18:
	MOVQ R8, DX          // <--                                  // mov	rdx, r8
	ANDQ $-0x80, DX      // <--                                  // and	rdx, -128
	LEAQ 0(AX)(DI*1), R9 // <--                                  // lea	r9, [rax + rdi]
	ADDQ $0x60, R9       // <--                                  // add	r9, 96
	LONG $0xc0eff9c5     // VPXOR X0, X0, X0                     // vpxor	xmm0, xmm0, xmm0
	XORL R10, R10        // <--                                  // xor	r10d, r10d
	LONG $0xc9eff1c5     // VPXOR X1, X1, X1                     // vpxor	xmm1, xmm1, xmm1
	LONG $0xd2efe9c5     // VPXOR X2, X2, X2                     // vpxor	xmm2, xmm2, xmm2
	LONG $0xdbefe1c5     // VPXOR X3, X3, X3                     // vpxor	xmm3, xmm3, xmm3

LBB0_19:
	LONG $0xeb7d81c4; WORD $0x1144; BYTE $0xa0 // VPOR -0x60(R9)(R10*1), Y0, Y0        // vpor	ymm0, ymm0, ymmword ptr [r9 + r10 - 96]
	LONG $0xeb7581c4; WORD $0x114c; BYTE $0xc0 // VPOR -0x40(R9)(R10*1), Y1, Y1        // vpor	ymm1, ymm1, ymmword ptr [r9 + r10 - 64]
	LONG $0xeb6d81c4; WORD $0x1154; BYTE $0xe0 // VPOR -0x20(R9)(R10*1), Y2, Y2        // vpor	ymm2, ymm2, ymmword ptr [r9 + r10 - 32]
	LONG $0xeb6581c4; WORD $0x111c             // VPOR 0(R9)(R10*1), Y3, Y3            // vpor	ymm3, ymm3, ymmword ptr [r9 + r10]
	SUBQ $-0x80, R10                           // <--                                  // sub	r10, -128
	CMPQ DX, R10                               // <--                                  // cmp	rdx, r10
	JNE  LBB0_19                               // <--                                  // jne	.LBB0_19
	LONG $0xc0ebf5c5                           // VPOR Y0, Y1, Y0                      // vpor	ymm0, ymm1, ymm0
	LONG $0xcaebe5c5                           // VPOR Y2, Y3, Y1                      // vpor	ymm1, ymm3, ymm2
	LONG $0xc0ebf5c5                           // VPOR Y0, Y1, Y0                      // vpor	ymm0, ymm1, ymm0
	LONG $0x397de3c4; WORD $0x01c1             // VEXTRACTI128 $0x1, Y0, X1            // vextracti128	xmm1, ymm0, 1
	LONG $0xc1ebf9c5                           // VPOR X1, X0, X0                      // vpor	xmm0, xmm0, xmm1
	LONG $0xc870f9c5; BYTE $0xee               // VPSHUFD $-0x12, X0, X1               // vpshufd	xmm1, xmm0, 238
	LONG $0xc1ebf9c5                           // VPOR X1, X0, X0                      // vpor	xmm0, xmm0, xmm1
	LONG $0xc870f9c5; BYTE $0x55               // VPSHUFD $0x55, X0, X1                // vpshufd	xmm1, xmm0, 85
	LONG $0xc1ebf9c5                           // VPOR X1, X0, X0                      // vpor	xmm0, xmm0, xmm1
	LONG $0xd072f1c5; BYTE $0x10               // VPSRLD $0x10, X0, X1                 // vpsrld	xmm1, xmm0, 16
	LONG $0xc1ebf9c5                           // VPOR X1, X0, X0                      // vpor	xmm0, xmm0, xmm1
	LONG $0xd071f1c5; BYTE $0x08               // VPSRLW $0x8, X0, X1                  // vpsrlw	xmm1, xmm0, 8
	LONG $0xc1ebf9c5                           // VPOR X1, X0, X0                      // vpor	xmm0, xmm0, xmm1
	LONG $0x7e79c1c4; BYTE $0xc1               // VMOVD X0, R9                         // vmovd	r9d, xmm0
	CMPQ R8, DX                                // <--                                  // cmp	r8, rdx
	JE   LBB0_24                               // <--                                  // je	.LBB0_24
	LONG $0x78c0f641                           // TESTL $0x78, R8                      // test	r8b, 120
	JE   LBB0_22                               // <--                                  // je	.LBB0_22

LBB0_15:
	MOVQ R8, R10                 // <--                                  // mov	r10, r8
	ANDQ $-0x8, R10              // <--                                  // and	r10, -8
	LONG $0xc9b60f45             // MOVZX R9, R9                         // movzx	r9d, r9b
	LONG $0x6e79c1c4; BYTE $0xc1 // VMOVD R9, X0                         // vmovd	xmm0, r9d
	LEAQ 0(DI)(AX*1), R9         // <--                                  // lea	r9, [rdi + rax]
	ADDQ R10, AX                 // <--                                  // add	rax, r10

LBB0_16:
	LONG $0x7e7ac1c4; WORD $0x110c // VMOVQ 0(R9)(DX*1), X1                // vmovq	xmm1, qword ptr [r9 + rdx]
	LONG $0xc0ebf1c5               // VPOR X0, X1, X0                      // vpor	xmm0, xmm1, xmm0
	ADDQ $0x8, DX                  // <--                                  // add	rdx, 8
	CMPQ R10, DX                   // <--                                  // cmp	r10, rdx
	JNE  LBB0_16                   // <--                                  // jne	.LBB0_16
	LONG $0xc870f9c5; BYTE $0x55   // VPSHUFD $0x55, X0, X1                // vpshufd	xmm1, xmm0, 85
	LONG $0xc1ebf9c5               // VPOR X1, X0, X0                      // vpor	xmm0, xmm0, xmm1
	LONG $0xd072f1c5; BYTE $0x10   // VPSRLD $0x10, X0, X1                 // vpsrld	xmm1, xmm0, 16
	LONG $0xc1ebf9c5               // VPOR X1, X0, X0                      // vpor	xmm0, xmm0, xmm1
	LONG $0xd071f1c5; BYTE $0x08   // VPSRLW $0x8, X0, X1                  // vpsrlw	xmm1, xmm0, 8
	LONG $0xc1ebf9c5               // VPOR X1, X0, X0                      // vpor	xmm0, xmm0, xmm1
	LONG $0x7e79c1c4; BYTE $0xc1   // VMOVD X0, R9                         // vmovd	r9d, xmm0
	CMPQ R8, R10                   // <--                                  // cmp	r8, r10
	JNE  LBB0_23                   // <--                                  // jne	.LBB0_23
	JMP  LBB0_24                   // <--                                  // jmp	.LBB0_24

LBB0_22:
	ADDQ DX, AX // <--                                  // add	rax, rdx

LBB0_23:
	ORB  0(DI)(AX*1), R9 // <--                                  // or	r9b, byte ptr [rdi + rax]
	INCQ AX              // <--                                  // inc	rax
	CMPQ SI, AX          // <--                                  // cmp	rsi, rax
	JNE  LBB0_23         // <--                                  // jne	.LBB0_23

LBB0_24:
	LONG $0x80e18041 // ANDL $0x80, R9                       // and	r9b, -128
	LONG $0xd1b60f41 // MOVZX R9, DX                         // movzx	edx, r9b

LBB0_25:
	WORD $0xca09             // ORL CX, DX                           // or	edx, ecx
	WORD $0x940f; BYTE $0xc0 // SETE AL                              // sete	al
	NOP                      // (skipped)                            // mov	rsp, rbp
	NOP                      // (skipped)                            // pop	rbp
	VZEROUPPER               // <--                                  // vzeroupper
	MOVB AX, ret+16(FP)      // <--
	RET                      // <--                                  // ret
